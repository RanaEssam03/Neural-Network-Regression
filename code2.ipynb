import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

features_names = [
    'cement', 'water', 'superplasticizer', 'age'   
]
target_name = 'concrete_compressive_strength'
# Load the data
data = pd.read_csv('concrete_data.csv')

# print(data.head())
# print("Headers:", data.columns.tolist())
# print("Data shape:", data.shape)

x = data[features_names]
y = data[target_name]

scaller = StandardScaler()

def normalize(data,columns, is_test=False):
    if is_test:
        data = scaller.transform(data)
    else:
        data = scaller.fit_transform(data)
    return pd.DataFrame(data, columns=columns)
      
   
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42, shuffle=True)
x_train = normalize(x_train, features_names)
x_test = normalize(x_test, is_test=True, columns=features_names)
y_train = normalize(y_train.values.reshape(-1, 1), columns=[target_name])
y_test = normalize(y_test.values.reshape(-1, 1), is_test=True, columns=[target_name])

# print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, epochs=1000, learning_rate=0.01):
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.init_weights()

    def init_weights(self):
      self.weights1 = np.random.uniform(-0.5, 0.5, (self.input_size, self.hidden_size))
      self.weights2 = np.random.uniform(-0.5, 0.5, (self.hidden_size, self.output_size))
      self.hidden_layer_bias = np.random.uniform(-0.5, 0.5, (1, self.hidden_size))
      self.output_layer_bias = np.random.uniform(-0.5, 0.5, (1, self.output_size))
        # self.hidden_layer_bias = np.ones((1, self.hidden_size))
        # self.output_layer_bias = np.ones((1, self.output_size))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)

    def forward(self, x):
        # Hidden layer forward pass
        hidden_layer_input = np.dot(x, self.weights1) + self.hidden_layer_bias
        hidden_layer_output = self.sigmoid(hidden_layer_input)
        
        # Output layer forward pass (linear activation)
        output_layer_input = np.dot(hidden_layer_output, self.weights2) + self.output_layer_bias
        output_layer_output = output_layer_input  # Linear activation
        return output_layer_output, hidden_layer_output

    def backward(self, x, y, output, hidden_layer_output):
        # Step 1: Calculate output error and delta (for linear activation, delta = error)
        output_error = y - output
        output_layer_delta = output_error  # Derivative of linear activation is 1

        # Step 3: Backpropagate error to hidden layer
        hidden_layer_error = np.dot(output_layer_delta, self.weights2.T)
        hidden_layer_delta = self.sigmoid_derivative(hidden_layer_output) * hidden_layer_error

        # Step 2: Update output layer weights and biases
        self.weights2 += self.learning_rate * np.dot(hidden_layer_output.T, output_layer_delta)
        self.output_layer_bias += self.learning_rate * np.sum(output_layer_delta, axis=0, keepdims=True)

        # Step 4: Update hidden layer weights and biases
        self.weights1 += self.learning_rate * np.dot(x.T, hidden_layer_delta)
        self.hidden_layer_bias += self.learning_rate * np.sum(hidden_layer_delta, axis=0, keepdims=True)

    def train(self, x, y, patience=100):
        epochs_without_improvement = 0
        best_error = float('inf')
        x = np.array(x)
        y = np.array(y).reshape(-1, 1)  # Ensure y is column vector
        for i in range(self.epochs):
            # Forward pass
            final_output, hidden_layer_output = self.forward(x)

            # Backward pass
            self.backward(x, y, final_output, hidden_layer_output)

            # Calculate error
            error = np.mean(0.5 * ((y - final_output) ** 2))
            if i % 100 == 0:
                print(f"Epoch {i+1}, Loss: {error}")
            
            # if error < best_error:
            #     best_error = error
            #     epochs_without_improvement = 0
            # else:
            #     epochs_without_improvement += 1
            #     if epochs_without_improvement == patience:
            #         print(f"Training stopped as error has not improved in {patience} epochs.")
            #         break

    def predict(self, x):
        x = np.array(x)
        return self.forward(x)[0]


# Example Usage
nn = NeuralNetwork(input_size=4, hidden_size=8, output_size=1, epochs=1000, learning_rate=0.01)

# Train the network
nn.train(x_train, y_train)

# Predict and evaluate
from sklearn.metrics import r2_score

predictions = nn.predict(x_test)

# Calculate Mean Squared Error
mse_test = np.mean(0.5 * ((y_test - predictions) ** 2))

# Calculate R^2 Score
r2 = r2_score(y_test, predictions)

accuracy = 1 - mse_test

print(f"Test MSE: {mse_test}")
print(f"Test Accuracy: {accuracy}")
print(f"R^2 Score: {r2}")
